{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_S01_MH_Word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhered/nlp-de-cero-a-cien/blob/embeddings/NLP_S01_MH_Word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvNCXd9Dfqe1"
      },
      "source": [
        "# Word2vec con Gensim\n",
        "\n",
        "En este cuaderno de Jupyter vas a utilizar la biblioteca [Gensim](https://radimrehurek.com/gensim/index.html) para experimentar con word2vec. Este cuaderno está enfocado en la intuición de los conceptos y no en los detalles de implementación. Este cuaderno está inspirado en esta [guía](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html).\n",
        "\n",
        "## 1. Instalación y cargar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKIqnDXXfpiz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "052208e5-334a-43ac-afa4-8bbac717964a"
      },
      "source": [
        "!pip install --upgrade gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/52/f1417772965652d4ca6f901515debcd9d6c5430969e8c02ee7737e6de61c/gensim-4.0.1-cp37-cp37m-manylinux1_x86_64.whl (23.9MB)\n",
            "\u001b[K     |████████████████████████████████| 23.9MB 126kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.1.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn7Q2jB3frOn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "472f7795-3ad0-4b2d-b05a-2e58a9f2c37d"
      },
      "source": [
        "import gensim.downloader as api"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
            "  warnings.warn(msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBaT8djWkaZy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "470dcc51-a551-4f4c-a4f1-360265e830b3"
      },
      "source": [
        "model = api.load('word2vec-google-news-300')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVZm7iTOoawW"
      },
      "source": [
        "## 2. Similitud de palabras\n",
        "\n",
        "En esta sección veremos cómo conseguir la similitud entre dos palabras utilizando un word embedding ya entrenado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOZfaelLoi4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75e2e3d8-d64a-423e-80fb-a28676e68f46"
      },
      "source": [
        "model.similarity(\"king\", \"queen\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6510957"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX-Kk9HZofuF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e531a00-cb99-4e09-fbe1-b6c5d147c33b"
      },
      "source": [
        "model.similarity(\"king\", \"man\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.22942673"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypFK-pLrol3N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d0a6d0c-75fd-4c1d-842e-b3f6e6b8d3f2"
      },
      "source": [
        "model.similarity(\"king\", \"potato\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09978465"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBWzZySFormq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59439d53-ad2b-4a35-b4ee-9e8a0c9d3c57"
      },
      "source": [
        "model.similarity(\"king\", \"king\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GijWs_tx83W"
      },
      "source": [
        "Ahora veremos cómo encontrar las palabras con mayor similitud al conjunto de palabras especificado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytELAWBLk2-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "435e9736-6993-42f9-9b5f-b0d96e8671ad"
      },
      "source": [
        "model.most_similar([\"king\", \"queen\"], topn=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('monarch', 0.7042067050933838),\n",
              " ('kings', 0.6780861616134644),\n",
              " ('princess', 0.6731551885604858),\n",
              " ('queens', 0.6679497957229614),\n",
              " ('prince', 0.6435247659683228)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D4ZS7N3ovxB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b47d21c-af78-455c-b2ee-e3ccf53d617c"
      },
      "source": [
        "model.most_similar([\"tomato\", \"carrot\"], topn=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('carrots', 0.7536594867706299),\n",
              " ('tomatoes', 0.7129638195037842),\n",
              " ('celery', 0.7025030851364136),\n",
              " ('broccoli', 0.6796350479125977),\n",
              " ('cherry_tomatoes', 0.662927508354187)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZFlxKjOyBpu"
      },
      "source": [
        "Pero incluso puedes hacer cosas interesantes como ver qué palabra no corresponde a una lista."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CrZdcBpn3pn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "161570f2-d9cf-4114-db53-29b9a3447c76"
      },
      "source": [
        "model.doesnt_match([\"summer\", \"fall\", \"spring\", \"air\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'air'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko09hZ3dqMZ1"
      },
      "source": [
        "## Ejercicios\n",
        "\n",
        "1. Usa el modelo word2vec para hacer un ranking de las siguientes 15 palabras según su similitud con las palabras \"man\" y \"woman\". Para cada par, imprime su similitud."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzZ1eD3PpT-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b958f60f-9f59-44a4-82d9-9aba3fb149ef"
      },
      "source": [
        "words = [\n",
        "\"wife\",\n",
        "\"husband\",\n",
        "\"child\",\n",
        "\"queen\",\n",
        "\"king\",\n",
        "\"man\",\n",
        "\"woman\",\n",
        "\"birth\",\n",
        "\"doctor\",\n",
        "\"nurse\",\n",
        "\"teacher\",\n",
        "\"professor\",\n",
        "\"engineer\",\n",
        "\"scientist\",\n",
        "\"president\"]\n",
        "lst = []\n",
        "ref = [\"man\",\"woman\"]\n",
        "for word in words:\n",
        "  a = [model.similarity(ref[0], word), model.similarity(ref[1], word)] \n",
        "  lst.append((word, a[0], a[1]))\n",
        "lst_by_sim_man = sorted(lst, key=lambda x: x[1], reverse=True)\n",
        "lst_by_sim_woman = sorted(lst, key=lambda x: x[2], reverse=True)\n",
        "\n",
        "print(\"By similarity to man\")\n",
        "print(lst_by_sim_man)\n",
        "print(\"By similarity to woman\")\n",
        "print(lst_by_sim_woman)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "By similarity to man\n",
            "[('man', 1.0, 0.76640123), ('woman', 0.76640123, 1.0), ('husband', 0.34499747, 0.49281383), ('wife', 0.32920915, 0.444824), ('child', 0.31633338, 0.47500372), ('doctor', 0.31448963, 0.37945858), ('nurse', 0.2547229, 0.44135594), ('teacher', 0.25000125, 0.31357846), ('king', 0.22942673, 0.12847973), ('queen', 0.16658202, 0.31618136), ('scientist', 0.15824963, 0.15486898), ('engineer', 0.15128928, 0.09435377), ('birth', 0.11078789, 0.21471293), ('professor', 0.09415862, 0.13077852), ('president', 0.028424604, 0.062676705)]\n",
            "By similarity to woman\n",
            "[('woman', 0.76640123, 1.0), ('man', 1.0, 0.76640123), ('husband', 0.34499747, 0.49281383), ('child', 0.31633338, 0.47500372), ('wife', 0.32920915, 0.444824), ('nurse', 0.2547229, 0.44135594), ('doctor', 0.31448963, 0.37945858), ('queen', 0.16658202, 0.31618136), ('teacher', 0.25000125, 0.31357846), ('birth', 0.11078789, 0.21471293), ('scientist', 0.15824963, 0.15486898), ('professor', 0.09415862, 0.13077852), ('king', 0.22942673, 0.12847973), ('engineer', 0.15128928, 0.09435377), ('president', 0.028424604, 0.062676705)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKamywnxqxJJ"
      },
      "source": [
        "**2. Completa las siguientes analogías por tu cuenta (sin usar el modelo)**\n",
        "\n",
        "a. king is to throne as judge is to court\n",
        "\n",
        "b. giant is to dwarf as genius is to idiot\n",
        "\n",
        "c. French is to France as Spaniard is to Spain\n",
        "\n",
        "d. bad is to good as sad is to happy\n",
        "\n",
        "e. nurse is to hospital as teacher is to school\n",
        "\n",
        "f. universe is to planet as house is to room"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcNRxHuZrXAM"
      },
      "source": [
        "**3. Ahora completa las analogías usando un modelo word2vec**\n",
        "\n",
        "Aquí hay un ejemplo de cómo hacerlo. Puedes resolver analogías como \"A es a B como C es a _\" haciendo A + C - B. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4kF08h4qhxM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a21304e3-deac-4eec-f9c1-e9cd4f0c0ca4"
      },
      "source": [
        "# man is to woman as king is to ___?\n",
        "model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('queen', 0.7118193507194519)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiPbbGsori48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30756530-0184-46f4-cbb4-48c9ccc6f886"
      },
      "source": [
        "# us is to burger as italy is to ___?\n",
        "model.most_similar(positive=[\"Mexico\", \"burger\"], negative=[\"USA\"], topn=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('taco', 0.6266060471534729)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViL2_FWuiLoY",
        "outputId": "38e1a664-3b3a-4303-a0e0-c6c29c021c99"
      },
      "source": [
        "\"\"\"\n",
        "a. king is to throne as judge is to court\n",
        "b. giant is to dwarf as genius is to idiot\n",
        "c. French is to France as Spaniard is to Spain\n",
        "d. bad is to good as sad is to happy\n",
        "e. nurse is to hospital as teacher is to school\n",
        "f. universe is to planet as house is to room\n",
        "\"\"\"\n",
        "words = [[\"king\", \"throne\", \"judge\", \"court\"],\n",
        "[\"giant\", \"dwarf\", \"genius\", \"idiot\"],\n",
        "[\"French\", \"France\", \"Spaniard\", \"Spain\"],\n",
        "[\"bad\", \"good\", \"sad\", \"happy\"],\n",
        "[\"nurse\", \"hospital\", \"teacher\", \"school\"],\n",
        "[\"universe\", \"planet\", \"house\", \"room\"]]\n",
        "for line in words:\n",
        "  pred = model.most_similar(positive=[line[2], line[1]], negative=[line[0]], topn=1)\n",
        "  print(line[0],\" is to \", line[1], \" as \", line[2], \" is to \", pred[0][0], \", (vs. \", line[3],\")\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "king  is to  throne  as  judge  is to  appellate_court , (vs.  court )\n",
            "giant  is to  dwarf  as  genius  is to  savant , (vs.  idiot )\n",
            "French  is to  France  as  Spaniard  is to  rider_Dani_Pedrosa , (vs.  Spain )\n",
            "bad  is to  good  as  sad  is to  wonderful , (vs.  happy )\n",
            "nurse  is to  hospital  as  teacher  is to  school , (vs.  school )\n",
            "universe  is to  planet  as  house  is to  bungalow , (vs.  room )\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}